{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e15f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.special import digamma, betaln, gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacfd9a0",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8762003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=[] # 每一封邮件的内容\n",
    "label=[] # 对应邮件内容的label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb92d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datas(directory):\n",
    "    ham_path=directory+'/ham'\n",
    "    spam_path=directory+'/spam'\n",
    "    # 读取邮件内容\n",
    "    for file_name in os.listdir(ham_path):    \n",
    "        path=ham_path+'/'+file_name\n",
    "        with open(path,'r',encoding='utf-8',errors='replace') as file:\n",
    "            content=file.read()\n",
    "            data_file.append(content)\n",
    "            label.append(0) # 0表示正常邮件\n",
    "    \n",
    "    for file_name in os.listdir(spam_path): \n",
    "        path=spam_path+'/'+file_name\n",
    "        with open(path,'r',encoding='utf-8',errors='replace') as file:\n",
    "            content=file.read()\n",
    "            data_file.append(content)\n",
    "            label.append(1) # 1表示垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93282e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集共27716封邮件\n",
      "其中,垃圾邮件有12671封,正常邮件有15045封\n"
     ]
    }
   ],
   "source": [
    "# 取前五个数据集作为总体数据集\n",
    "for str in ['enron1','enron2','enron3','enron4','enron5']:\n",
    "    directory='./dataset/'+str\n",
    "    read_datas(directory)\n",
    "\n",
    "print(f'数据集共{len(data_file)}封邮件')\n",
    "print(f'其中,垃圾邮件有{sum(label)}封,正常邮件有{len(label)-sum(label)}封')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e3861",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "969ca307",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set([\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'if', 'because', 'as', 'what',\n",
    "            'which', 'this', 'that', 'these', 'those', 'then', 'just', 'so', 'than',\n",
    "            'such', 'both', 'through', 'about', 'for', 'is', 'of', 'while', 'during',\n",
    "            'to', 'from', 'in', 'on', 'by', 'with', 'at', 'be', 'was', 'were', 'are','subject'\n",
    "        ])\n",
    "\n",
    "def clean_text(text):\n",
    "    text=text.lower()\n",
    "    text = re.sub(r'\\S+@\\S+', '', text) # 移除邮箱地址\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # 移除标点符号和数字\n",
    "    tokens = text.split() #分词\n",
    "    tokens = [token for token in tokens if token not in stop_words] #移除停用词\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed64cd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: christmas tree farm pictures\n",
      "\n",
      "christmas tree farm pictures\n",
      "                                                text  label\n",
      "0                       christmas tree farm pictures      0\n",
      "1  vastar resources inc gary production high isla...      0\n",
      "2  calpine daily gas nomination calpine daily gas...      0\n",
      "3  re issue fyi see note below already done stell...      0\n",
      "4  meter nov allocation fyi forwarded lauri allen...      0\n"
     ]
    }
   ],
   "source": [
    "token_file=[]\n",
    "for text in data_file:\n",
    "    token=clean_text(text)\n",
    "    token_file.append(token)\n",
    "print(data_file[0])\n",
    "print(token_file[0])\n",
    "\n",
    "datafile={'text':token_file,'label':label}\n",
    "data=pd.DataFrame(data=datafile)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb7f8d",
   "metadata": {},
   "source": [
    "## 特征提取与划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09dc4ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: (19401, 5000)\n",
      "测试集大小: (8315, 5000)\n"
     ]
    }
   ],
   "source": [
    "# 特征转换\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(data['text']).toarray()  # X: (n_samples, n_features)\n",
    "y = data['label'].values\n",
    "\n",
    "# 划分训练集/测试集（7:3）\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape}\")\n",
    "print(f\"测试集大小: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac7631",
   "metadata": {},
   "source": [
    "# 构建变分贝叶斯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cbb52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifier:\n",
    "    def __init__(self):\n",
    "        self.alpha0 = 1\n",
    "        self.beta0 = 1\n",
    "        self.lambda0 = 1\n",
    "        self.max_iter = 100\n",
    "        self.tol = 1e-6\n",
    "\n",
    "    def compute_elbo(self,alpha, beta, lambda0, lambda1, alpha0, beta0, lambda0_prior, X, y):\n",
    "        n, k = X.shape\n",
    "        n0 = np.sum(y==0)\n",
    "        n1 = np.sum(y==1)\n",
    "        \n",
    "        # 计算L1: pi的贡献\n",
    "        L1 = (alpha0-1)*digamma(alpha) + (beta0-1)*digamma(beta) - (alpha-1)*digamma(alpha) - (beta-1)*digamma(beta)\n",
    "        L1 -= betaln(alpha, beta) - betaln(alpha0, beta0)\n",
    "        \n",
    "        # 计算L2: theta0和theta1的贡献\n",
    "        sum_lambda0 = np.sum(lambda0)\n",
    "        sum_lambda1 = np.sum(lambda1)\n",
    "        L2 = 0\n",
    "        # 类别0\n",
    "        L2 += k*(lambda0_prior-1)*digamma(lambda0_prior) - (sum_lambda0 - k)*digamma(sum_lambda0)\n",
    "        L2 += np.sum((lambda0_prior - lambda0)*digamma(lambda0))\n",
    "        L2 += gammaln(k*lambda0_prior) - np.sum(gammaln(lambda0_prior)) - (gammaln(sum_lambda0) - np.sum(gammaln(lambda0)))\n",
    "        # 类别1\n",
    "        L2 += k*(lambda0_prior-1)*digamma(lambda0_prior) - (sum_lambda1 - k)*digamma(sum_lambda1)\n",
    "        L2 += np.sum((lambda0_prior - lambda1)*digamma(lambda1))\n",
    "        L2 += gammaln(k*lambda0_prior) - np.sum(gammaln(lambda0_prior)) - (gammaln(sum_lambda1) - np.sum(gammaln(lambda1)))\n",
    "        \n",
    "        # 计算L3: 标签似然的贡献\n",
    "        L3 = n1*digamma(alpha) + n0*digamma(beta) - n*digamma(alpha + beta)\n",
    "        \n",
    "        # 计算L4: 特征似然的贡献\n",
    "        L4 = 0\n",
    "        # 类别0\n",
    "        X0 = X[y==0]\n",
    "        L4 += np.sum(X0 * digamma(lambda0)[np.newaxis, :]) - n0 * np.sum(X0 * digamma(sum_lambda0))\n",
    "        # 类别1\n",
    "        X1 = X[y==1]\n",
    "        L4 += np.sum(X1 * digamma(lambda1)[np.newaxis, :]) - n1 * np.sum(X1 * digamma(sum_lambda1))\n",
    "        \n",
    "        return L1 + L2 + L3 + L4\n",
    "    \n",
    "    def train(self, X_train,y_train):\n",
    "        n, k = X_train.shape\n",
    "        n1 = np.sum(y_train)  # 垃圾邮件数\n",
    "        n0 = n - n1           # 非垃圾邮件数\n",
    "    \n",
    "        # 初始化变分参数\n",
    "        alpha = self.alpha0 + n1\n",
    "        beta = self.beta0 + n0\n",
    "        lambda0_j = self.lambda0 + np.sum(X_train[y_train==0], axis=0)  # 类别0的变分参数\n",
    "        lambda1_j = self.lambda0 + np.sum(X_train[y_train==1], axis=0)  # 类别1的变分参数\n",
    "    \n",
    "        # 计算初始ELBO\n",
    "        prev_elbo = self.compute_elbo(alpha, beta, lambda0_j, lambda1_j, self.alpha0, self.beta0, self.lambda0, X_train, y_train)\n",
    "    \n",
    "        for iter in range(self.max_iter):\n",
    "           # 更新变分参数（此处因共轭性，一次更新即可收敛，迭代仅为验证ELBO）\n",
    "            alpha_new = self.alpha0 + n1\n",
    "            beta_new = self.beta0 + n0\n",
    "            lambda0_j_new = self.lambda0 + np.sum(X_train[y_train==0], axis=0)\n",
    "            lambda1_j_new = self.lambda0 + np.sum(X_train[y_train==1], axis=0)\n",
    "            # 计算新ELBO\n",
    "            curr_elbo = self.compute_elbo(alpha_new, beta_new, lambda0_j_new, lambda1_j_new, self.alpha0, self.beta0, self.lambda0, X_train, y_train)\n",
    "            \n",
    "            # 检查收敛\n",
    "            if np.abs(curr_elbo - prev_elbo) < self.tol:\n",
    "                break\n",
    "            \n",
    "            # 更新参数\n",
    "            alpha, beta = alpha_new, beta_new\n",
    "            lambda0_j, lambda1_j = lambda0_j_new, lambda1_j_new\n",
    "            prev_elbo = curr_elbo\n",
    "        \n",
    "        return {'alpha': alpha, 'beta': beta, 'lambda0': lambda0_j, 'lambda1': lambda1_j}\n",
    "    \n",
    "    def predict(self,model, X_test, S=200):\n",
    "        alpha, beta = model['alpha'], model['beta']\n",
    "        lambda0, lambda1 = model['lambda0'], model['lambda1']\n",
    "        sum_lambda0, sum_lambda1 =  np.sum(model['lambda0']), np.sum(model['lambda1'])\n",
    "        \n",
    "        # 1. 计算变分参数的均值（解析解）\n",
    "        pi_mean = alpha / (alpha + beta)  # E[pi]\n",
    "        theta0_mean = lambda0 / sum_lambda0  # E[theta0_j]\n",
    "        theta1_mean = lambda1 / sum_lambda1  # E[theta1_j]\n",
    "        \n",
    "        # 2. 计算对数似然 log p(x|theta_c) = sum(x_j * log(theta_cj))（批量计算）\n",
    "        # 稀疏矩阵乘法：X_test (n_test, k) @ log_theta (k, 1) → (n_test, 1)\n",
    "        log_theta0 = np.log(theta0_mean + 1e-10)  # 加小值避免log(0)\n",
    "        log_theta1 = np.log(theta1_mean + 1e-10)\n",
    "        \n",
    "        # 批量计算所有测试样本的对数似然\n",
    "        log_p_x0 = X_test @ log_theta0.reshape(-1, 1)  # (n_test, 1)\n",
    "        log_p_x1 = X_test @ log_theta1.reshape(-1, 1)  # (n_test, 1)\n",
    "        \n",
    "        # 3. 计算后验概率 p(y=1|x) ≈ [pi_mean * exp(log_p_x1)] / [pi_mean*exp(...) + (1-pi_mean)*exp(...)]\n",
    "        # 取指数避免数值下溢：exp(a)/(exp(a)+exp(b)) = 1/(1+exp(b-a))\n",
    "        log_ratio = log_p_x1 - log_p_x0  # log(p(x|1)/p(x|0))\n",
    "        pi_ratio = pi_mean / (1 - pi_mean + 1e-10)  # 避免分母为0\n",
    "        log_posterior_ratio = np.log(pi_ratio) + log_ratio\n",
    "        \n",
    "        #  sigmoid函数转换为概率（数值稳定）\n",
    "        y_pred_prob = 1 / (1 + np.exp(-log_posterior_ratio))\n",
    "        y_pred_prob = y_pred_prob.flatten()  # 转为一维数组\n",
    "        \n",
    "        # 4. 阈值判断（默认0.5）\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "        return y_pred, y_pred_prob\n",
    "\n",
    "    def evaluate(self,model,X_test,y_test):\n",
    "        pred_y,_=self.predict(model, X_test)\n",
    "        accuracy=accuracy_score(y_test,pred_y)\n",
    "        print(f'准确率为{accuracy:.2f}')\n",
    "        cm=confusion_matrix(y_test,pred_y)\n",
    "        print(\"混淆矩阵为\")\n",
    "        print(cm)\n",
    "\n",
    "        return accuracy,pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97e543fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=SpamClassifier()\n",
    "model=classifier.train(X_train,y_train) # 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e67fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率为0.98\n",
      "混淆矩阵为\n",
      "[[4419   95]\n",
      " [  70 3731]]\n"
     ]
    }
   ],
   "source": [
    "accuracy,pred_y=classifier.evaluate(model,X_test,y_test) # 模型评估"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
